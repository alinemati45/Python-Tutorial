{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "* Ali Nemati.\n",
    "\n",
    "MAster computer science WA, Seattle, 2019\n",
    "\n",
    "Live in Wisconsin. \n",
    "\n",
    "Working with epidemiology team at mcw\n",
    "\n",
    "Update covid 19 analysis and report\n",
    "\n",
    "Manage and develop Milwaukee Homicide and Nonfatal Shooting Dashboards\n",
    "\n",
    "7 years experience at database\n",
    "\n",
    "4 years academia and industry data scientist \n",
    "\n",
    "Tableau 2 year experience.\n",
    "\n",
    "I work with python and R to analysis and statistic. \n",
    "\n",
    "I have several publications related to machine learning and deep learning in health area.\n",
    "\n",
    "My hubby is reading articles and watching movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CRISP-DM framework is comprised of 9 major steps:\n",
    "\n",
    "## Business understanding\n",
    "This entails the understanding of a project’s objectives and requirements from the business viewpoint. Such business perspectives are used to figure out what business problems to solve via the use of data mining.\n",
    "\n",
    "## Data understanding \n",
    "This phase allows us to become familiarize with the data and this involves performing exploratory data analysis. Such initial data exploration may allow us to figure out which subsets of data to use for further modeling as well as aid in the generation of hypothesis to explore.\n",
    "\n",
    "## Data preparation\n",
    "This can be considered to be the most time-consuming phase of the data mining process as it involves rigorous data cleaning and pre-processing as well as the handling of missing data.\n",
    "\n",
    "## Modelling \n",
    "The pre-processed data are used for model building in which learning algorithms are used to perform multivariate analysis.\n",
    "\n",
    "## Evaluation\n",
    "In performing the 4 aforementioned steps, it is important to evaluate the accrued results and review the process performed thusfar to determine whether the originally set business objectives are met or not. If deemed appropriate, some steps may need to be performed again. Rinse and repeat. Once it is deemed that the results and process are satisfactory then we are ready to move to deployment. Additionally, in this evaluation phase, some findings may ignite new project ideas for which to explore.\n",
    "\n",
    "## Deployment\n",
    "Once the model is of satisfactory quality, the model is then deployed, which may range from being a simple report, an API that can be accessed via programmatic calls, a web application, etc.\n",
    "Model optimization and get more data\n",
    "## Evaluation \n",
    "## Deployment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Name: Demand Forecasting for 16 weeks ahead at Kohls.\n",
    "Project Name : Safety Stock at Kohls\n",
    "Purpose: The objective is to be able to better forecast inventory needs across channels and locations using well-established historical, and structured sales data.\n",
    "\n",
    "Descriptions:\n",
    "\n",
    "- Created Python Scripts to automate pulling data from different data sources for data analysis.\n",
    "\n",
    "-Creating and modifying SQL queries to aggregate data.\n",
    "- Extracting factors that lead to higher sales for individual stores based on past-sales data.\n",
    "- Analyzed data from Teradata and Google BigQuery.\n",
    "- Reducing factors that lead to the cancellation of orders in Omni-fulfillment channels.\n",
    "\n",
    "-Implemented Machine Learning models and application that predicts and recommends levels of safety-stock and top store based on the percentage chance of an order being canceled.\n",
    "\n",
    "- Applying tradition models\n",
    "* Random Forest Regression  * Generalized Linear Regression * Gradient-boosted tree regression\n",
    "* Linear regression * Decision tree regression\n",
    "- Applying a deep learning model on PySpark: \n",
    "\n",
    "1.Random Forest Regression 2. Long short-term memory\n",
    "Metrics: \n",
    "R-squared , MSE , RMSE , MAE and SMAPE\n",
    "\n",
    "Prepare data and Data Cleaning:\n",
    "1.  Missing data like Null: (Imputation)\n",
    "Numerical Imputation\n",
    "Categorical imputation\n",
    "## Deal With Imputation\n",
    "\t1. Leave as it is\n",
    "    2. Filling the missing values: mode, median, mean \n",
    "    3. Drop them\n",
    "    4. Predicting The Missing Values\n",
    "    5. Using Algorithms Which Support Missing Values - KNN\n",
    "    6. Assigning An Unique Category\n",
    "\n",
    "\n",
    "## Detect outliers:\n",
    "        Box Plot, Scatter plot\n",
    "        Z-score \n",
    "        IQR score\n",
    "        Isolation Forest (Unsupervised machine learning)\n",
    "        Robust Random Cut Forest  (Unsupervised machine learning)\n",
    "\n",
    "##  Duplicate rows or irrelevant: How to remove them\n",
    "## Tidy data set: \n",
    "each column represents separate variables and each row represents individual observations. But in untidy data, each column represents values but not the variables. \n",
    "Using pandas. melt\n",
    "## Different types of variables: \n",
    "or Converting data types\n",
    "In DataFrame data can be of many types. As example :\n",
    "    1. Categorical data\n",
    "    2. Object data\n",
    "    3. Numeric data\n",
    "    4. Boolean data\n",
    "    6. String manipulation:  find a reason that store indicates to the company by applying regular expression for matching that pattern\n",
    "## Augment your data: \n",
    "reduce data redundancy and dimensionality from 2000 features to 250 features.\n",
    "## Data Concatenation: \n",
    "work with multiple tables in one table\n",
    "## Inconsistent column: \n",
    "change to two dimensional data structure. \n",
    "## Normalization : \n",
    "The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. \n",
    "    RobustScaler: RobustScaler transforms the feature vector by subtracting the median and then dividing by the interquartile range (75% value — 25% value)\n",
    "    MinMaxScaler: This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "    StandardScaler  : StandardScaler makes the mean of the distribution 0. About 68% of the values will lie be between -1 and 1.\n",
    "    \n",
    "## Normalizer: \n",
    "Normalizer works on the rows, not the columns! I find that very unintuitive. It’s easy to miss this information in the docs. \n",
    "    By default, L2 normalization is applied to each observation so the that the values in a row have a unit norm. Unit norm with L2 means that if each element were squared and summed, the total would equal 1. Alternatively, L1 (aka taxicab or Manhattan) normalization can be applied instead of L2 normalization.\n",
    "## Preprocessing: \n",
    "LabelEncoder , OneHotEncoder\n",
    "## Feature Engineering for Time Series #2: \n",
    "Time-Based Features\n",
    "    extract more granular features if we have the time stamp\n",
    "    For count feature: lag feature 1 , .. , 7\n",
    "Rolling Window Feature from mean, max , min and sum\n",
    "Expanding Window Feature from mean, max , min and sum\n",
    "\n",
    "## Data Visualization\n",
    "Using tableau to visualize our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 Python Libraries for Data Science\n",
    "*  TensorFlow and Keras  :  deep learning and neural network modules with GPU\n",
    "*  NumPy : numerical computation in Python\n",
    "*  SciPy : high-level computations\n",
    "*  Pandas: General data wrangling and data cleaning\n",
    "*  Matplotlib : beautiful visualizations\n",
    "*  SciKit-Learn : almost all the machine learning algorithms and metrics\n",
    "*  BeautifulSoup: web crawling and data scraping\n",
    "*  seaborn\n",
    "*  Popular Classification Algorithms:\n",
    "*  Logistic Regression\n",
    "*  Naive Bayes\n",
    "*  K-Nearest Neighbors\n",
    "*  Decision Tree\n",
    "*  Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 popular techniques for data dimensionality reduction:\n",
    "## Missing Values Ratio. \n",
    "Data columns with too many missing values are unlikely to carry much useful information. Thus, data columns with a ratio of missing values greater than a given threshold can be removed. The higher the threshold, the more aggressive the reduction.\n",
    "## Low Variance Filter. \n",
    "Similar to the previous technique, data columns with little changes in the data carry little information. Thus, all data columns with a variance lower than a given threshold can be removed. Notice that the variance depends on the column range, and therefore normalization is required before applying this technique.\n",
    "## High Correlation Filter. \n",
    "Data columns with very similar trends are also likely to carry very similar information, and only one of them will suffice for classification. Here we calculate the Pearson product-moment correlation coefficient between numeric columns and the Pearson’s chi-square value between nominal columns. For the final classification, we only retain one column of each pair of columns whose pairwise correlation exceeds a given threshold. Notice that correlation depends on the column range, and therefore, normalization is required before applying this technique.\n",
    "## Random Forests/Ensemble Trees. \n",
    "Decision tree ensembles, often called random forests, are useful for column selection in addition to being effective classifiers. Here we generate a large and carefully constructed set of trees to predict the target classes and then use each column’s usage statistics to find the most informative subset of columns. We generate a large set (2,000) of very shallow trees (two levels), and each tree is trained on a small fraction (three columns) of the total number of columns. If a column is often selected as the best split, it is very likely to be an informative column that we should keep. For all columns, we calculate a score as the number of times that the column was selected for the split, divided by the number of times in which it was a candidate. The most predictive columns are those with the highest scores.\n",
    "## Principal Component Analysis (PCA). \n",
    "Principal component analysis (PCA) is a statistical procedure that orthogonally transforms the original n numeric dimensions of a dataset into a new set of n dimensions called principal components. As a result of the transformation, the first principal component has the largest possible variance; each succeeding principal component has the highest possible variance under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding principal components. Keeping only the first m < n principal components reduces the data dimensionality while retaining most of the data information, i.e., variation in the data. Notice that the PCA transformation is sensitive to the relative scaling of the original columns, and therefore, the data need to be normalized before applying PCA. Also notice that the new coordinates (PCs) are not real, system-produced variables anymore. Applying PCA to your dataset loses its interpretability. If interpretability of the results is important for your analysis, PCA is not the transformation that you should apply.\n",
    "## Backward Feature Elimination. \n",
    "In this technique, at a given iteration, the selected classification algorithm is trained on n input columns. Then we remove one input column at a time and train the same model on n-1 columns.  The input column whose removal has produced the smallest increase in the error rate is removed, leaving us with n-1 input columns. The classification is then repeated using n-2 columns, and so on. Each iteration k produces a model trained on n-k columns and an error rate e(k). By selecting the maximum tolerable error rate, we define the smallest number of columns necessary to reach that classification performance with the selected machine learning algorithm.\n",
    "## Forward Feature Construction. \n",
    "This is the inverse process to backward feature elimination. We start with one column only, progressively adding one column at a time, i.e., the column that produces the highest increase in performance. Both algorithms, backward feature elimination and forward feature construction, are quite expensive in terms of time and computation. They are practical only when applied to a dataset with an already relatively low number of input columns.\n",
    "## Linear Discriminant Analysis (LDA)\n",
    "A number m of linear combinations (discriminant functions) of the n input features, with m < n, are produced to be uncorrelated and to maximize class separation. These discriminant functions become the new basis for the dataset. All numeric columns in the dataset are projected onto these linear discriminant functions, effectively moving the dataset from the n-dimensionality to the m-dimensionality.\n",
    "\n",
    "In order to apply the LDA technique for dimensionality reduction, the target column has to be selected first. The maximum number of reduced dimensions m is the number of classes in the target column minus one, or if smaller, the number of numeric columns in the data. Notice that linear discriminant analysis assumes that the target classes follow a multivariate normal distribution with the same variance but with a different mean for each class.\n",
    "## Autoencoder\n",
    "An autoencoder is a neural network, with as many n output units as input units, at least one hidden layer with m units where m < n, and trained with the backpropagation algorithm to reproduce the input vector onto the output layer. It reduces the numeric columns in the data by using the output of the hidden layer to represent the input vector.\n",
    "\n",
    "The first part of the autoencoder — from the input layer to the hidden layer of m units — is called the encoder. It compresses the n dimensions of the input dataset into an m-dimensional space. The second part of the autoencoder — from the hidden layer to the output layer — is known as the decoder. The decoder expands the data vector from an m-dimensional space into the original n-dimensional dataset and brings the data back to their original values.\n",
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "This technique reduces the n numeric columns in the dataset to fewer dimensions m (m < n) based on nonlinear local relationships among the data points. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points in the new lower dimensional space.\n",
    "\n",
    "In the first step, the data points are modeled through a multivariate normal distribution of the numeric columns. In the second step, this distribution is replaced by a lower dimensional t-distribution, which follows the original multivariate normal distribution as closely as possible. The t-distribution gives the probability of picking another point in the dataset as a neighbor to the current point in the lower dimensional space. The perplexity parameter controls the density of the data as the “effective number of neighbors for any point.” The greater the value of the perplexity, the more global structure is considered in the data. The t-SNE technique works only on the current dataset. It is not possible to export the model to apply it to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning Methods\n",
    "Decision trees have been around for a long time and also known to suffer from bias and variance.\n",
    "You will have a large bias with simple trees and a large variance with complex trees.\n",
    "## Ensemble methods \n",
    "which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner.\n",
    "### What is Ensemble Learning? Wisdom of the crowd\n",
    "Combine multiple weak models/learners into one predictive model to reduce bias, variance and/or improve accuracy.\n",
    "# Types of Ensemble Learning: N number of weak learners\n",
    "## Bagging: \n",
    "Trains N different weak models (usually of same types – homogenous) with N non-overlapping subset of the\n",
    "input dataset in parallel. In the test phase, each model is evaluated. The label with the greatest number of predictions is selected as the prediction. Bagging methods reduces variance of the prediction.\n",
    "![](./i/image8.png)\n",
    "Random Forest is an ensemble machine learning algorithm that follows the bagging technique.\n",
    "Bagging (Bootstrap Aggregation) is used when our goal is to reduce the variance of a decision tree.\n",
    "Here the idea is to create several subsets of data from the training sample chosen randomly with\n",
    "replacement. Now, each collection of subset data is used to train their decision trees. As a result, we\n",
    "end up with an ensemble of different models. Average of all the predictions from different trees are\n",
    "used which is more robust than a single decision tree.\n",
    "## Boosting: \n",
    "Trains N different weak models (usually of same types – homogenous) with the complete dataset in a sequential order. The datapoints wrongly classified with previous weak model is provided more weights to that they can be classified by the next weak leaner properly. In the test phase, each model is evaluated and based on the test error of each weak model, the prediction is weighted for voting. Boosting methods decreases the bias of the prediction. Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample), and at every step, the goal is to solve for net error from the prior tree.\n",
    "When a hypothesis misclassified an input, its weight is increased, so that the next hypothesis is more\n",
    "likely to classify it correctly. By combining the whole set at the end converts weak learners into a\n",
    "better performing model. \n",
    "\n",
    "The different types of boosting algorithms are:\n",
    "1. AdaBoost\n",
    "2. Gradient Boosting\n",
    "3. XGBoost\n",
    "![](./i/image49.png)\n",
    "\n",
    "## Stacking: \n",
    "Trains N different weak models (usually of different types – heterogenous) with one of the two subsets of the\n",
    "dataset in parallel. Once the weak learners are trained, they are used to trained a meta learner to combine their\n",
    "predictions and carry out final prediction using the other subset. In test phase, each model predicts its label, these set of labels are fed to the meta learner which generates the final prediction\n",
    "![](./i/image5.png)\n",
    "\n",
    "\n",
    "![](./i/image57.png)\n",
    "\n",
    "\n",
    "![](./i/image22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Regularization in ML?\n",
    "* Regularization is an approach to address over-fitting in ML.\n",
    "* Overfitted model fails to generalize estimations on test data\n",
    "*When the underlying model to be learned is low bias/high variance, or when we have small amount of data, the estimated model is prone to overfitting.\n",
    "**Regularization reduces the variance of the model**\n",
    "## Types of Regularization: \n",
    "The main objective of creating a model(training data) is making sure it fits the data properly and reduce\n",
    "the loss. Sometimes the model that is trained which will fit the data but it may fail and give a poor\n",
    "performance during analyzing of data (test data). This leads to overfitting. Regularization came to\n",
    "overcome overfitting.\n",
    "\n",
    "\n",
    "### Modify the loss function:\n",
    "#### L1 Regularization: (Lasso Regression) : \n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “Absolute value of magnitude” of coefficient, as penalty term to the loss function. Lasso shrinks the less important feature’s coefficient to zero; thus, removing some features altogether. So, this works well for feature selection in case we have a huge number of features.\n",
    "![](./i/image51.png)\n",
    "Methods like Cross-validation, Stepwise Regression are there to handle overfitting and perform feature selection work well with a small set of features. These techniques are good when we are dealing with a large set of features.\n",
    "Along with shrinking coefficients, the lasso performs feature selection, as well. (Remember the\n",
    "‘selection‘ in the lasso full-form?) Because some of the coefficients become exactly zero, which is\n",
    "equivalent to the particular feature being excluded from the model.\n",
    "\n",
    "#### L2 Regularization: (Ridge Regression)\n",
    "\n",
    "Overfitting happens when the model learns signal as well as noise in the training data and wouldn’t perform well on new/unseen data on which model wasn’t trained on.\n",
    "To avoid overfitting your model on training data like cross-validation sampling, reducing the number of features, pruning, regularization, etc. So to avoid overfitting, we perform Regularization.\n",
    "![](./i/image59.png)\n",
    "The key difference between these two is the penalty term. \n",
    "![](./i/image24.png)\n",
    "The Regression model that uses L2 regularization is called Ridge Regression.\n",
    "* The formula for Ridge Regression:\n",
    "![](./i/image2.png)\n",
    "Regularization adds the penalty as model complexity increases. The regularization parameter\n",
    "(lambda) penalizes all the parameters except intercept so that the model generalizes the data and\n",
    "won’t overfit.\n",
    "\n",
    "Ridge regression adds “squared magnitude of the coefficient\" as penalty term to the loss\n",
    "function. Here the box part in the above image represents the L2 regularization element/term.\n",
    "![](./i/image9.png)\n",
    "\n",
    "* Lambda is a hyperparameter.\n",
    "If lambda is zero, then it is equivalent to OLS. But if lambda is very large, then it will add too much\n",
    "weight, and it will lead to under-fitting.\n",
    "Ridge regularization forces the weights to be small but does not make them zero and does not give\n",
    "the sparse solution.\n",
    "Ridge is not robust to outliers as square terms blow up the error differences of the outliers, and the regularization term tries to fix it by penalizing the weights\n",
    "Ridge regression performs better when all the input features influence the output, and all with weights are of roughly equal size.\n",
    "\n",
    "L2 regularization can learn complex data patterns.\n",
    "\n",
    "\n",
    "### Modify data sampling:\n",
    "#### Data augmentation: \n",
    "Create more data from available data by randomly cropping, dilating, rotating, adding small amount of noise etc.\n",
    "#### K-fold Cross-validation: \n",
    "Divide the data into k groups. Train on (k-1) groups and test on 1\n",
    "group. Try all k possible combinations\n",
    "### Change training approach:\n",
    "#### Injecting noise: \n",
    "Add random noise to the weights when they are being learned. It pushes the model to be relatively insensitive to small variations in the weights, hence regularization\n",
    "#### Dropout: \n",
    "Generally used for neural networks. Connections between consecutive layers are randomly dropped based on a dropout-ratio and the remaining network is trained in the current iteration. In the next iteration, another set of random connections are dropped.\n",
    "![](./i/image31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics:\n",
    "## What is the Confusion Matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n",
    "This is the key to the confusion matrix.\n",
    "It gives us insight not only into the errors being made by a classifier but, more importantly, the types of errors that are being made.\n",
    "**Positive (P):** Observation is positive (for example: is an apple).\n",
    "**Negative (N):** Observation is not positive (for example: is not an apple).\n",
    "**True positive(TP):** Sick people correctly identified as sick\n",
    "**False positive(FP):** Healthy people incorrectly identified as sick\n",
    "**True negative(TN):** Healthy people correctly identified as healthy\n",
    "**False negative(FN):** Sick people incorrectly identified as healthy\n",
    "In general, Positive = identified and negative = rejected. \n",
    "![](./i/image27.png)\n",
    "## Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "However, there are problems with accuracy. It assumes equal costs for both kinds of\n",
    "errors. A 99% accuracy can be excellent, good, mediocre, poor, or terrible depending upon\n",
    "the problem.\n",
    "\n",
    "\n",
    "## Misclassification Rate\n",
    "Misclassification Rate is defined as the ratio of the sum of False Positive and False\n",
    "Negative by Total(TP+TN+FP+FN)\n",
    "Misclassification Rate is also called Error Rate\n",
    "\n",
    "\n",
    "## Precision: tp / (tp + fp) :  \n",
    "specificity, selectivity or true negative rate (TNR)\n",
    "0 worst. 1 best\n",
    "Idea: How much “junk” we gave to the user? Or how many selected item are relevant.\n",
    "To get the value of precision, we divide the total number of correctly classified positive examples\n",
    "by the total number of predicted positive examples.\n",
    "1. High Precision indicates an example labeled as positive is indeed positive (a small number\n",
    "of FP).\n",
    "2. Low Precision indicates an example labeled as positive is indeed positive (large number of\n",
    "FP).\n",
    "If a tag has low precision, that means that texts from other tags are getting confused with the tag in question.\n",
    "## Recall:  tp / (tp + fn)\n",
    "\n",
    "sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "0 worst. 1 best\n",
    "Idea: How much ”good” stuff we missed? how many relevant item are selected\n",
    "Recall can be defined as the ratio of the total number of correctly classified positive examples\n",
    "divide to the total number of positive examples.\n",
    "1. High Recall indicates the class is correctly recognized (small number of FN).\n",
    "2. Low Recall indicates the class is incorrectly recognized (large number of FN)\n",
    "If a tag has low precision, that means that texts from other tags are getting confused with the tag in question.\n",
    "\n",
    "Since we have two measures (Precision and Recall), it helps to have a measurement that represents\n",
    "both of them. We calculate an F-measure, which uses Harmonic Mean in place of Arithmetic\n",
    "Mean as it punishes the extreme values more.\n",
    "The F-Measure will always be nearer to the smaller value of Precision or Recall.\n",
    "\n",
    "### Low recall, low precision\n",
    "![](./i/image52.png)\n",
    "### High recall, low precision\n",
    "![](./i/image35.png)\n",
    "\n",
    "### Low recall, high precision\n",
    "![](./i/image53.png)\n",
    "### High recall, high precision\n",
    "![](./i/image13.png)\n",
    "\n",
    "## F1-score = 2 × (precision × recall)/(precision + recall)\n",
    "0 worst. 1 best\n",
    "that combines recall and precision by taking their harmonic mean.\n",
    "![](./i/image58.png)\n",
    "\n",
    "## ROC \n",
    "tells how much model is capable of distinguishing between classes. Higher the AUC,\n",
    "better the model is at predicting 0s as 0s and 1s as 1s.\n",
    "![](./i/image25.png)\n",
    "\n",
    "![](./i/image21.png)\n",
    "\n",
    "## Two type error:\n",
    "Type 1 error: False positive\n",
    "The test incorrectly rejects the true null hypothesis that the patient is HIV negative. \n",
    "Type II error: False negative \n",
    "It occurs when a researcher fails to reject a null hypothesis which is really false\n",
    "![](./i/image10.png)\n",
    "\n",
    "## K fold Cross-validation \n",
    "is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "1. Take the group as a hold out or test data set\n",
    "2. Take the remaining groups as a training data set\n",
    "3. Fit a model on the training set and evaluate it on the test set\n",
    "4. Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.\n",
    "![](./i/image14.png)\n",
    "\n",
    "\n",
    "## Imbalanced Data in Classification\n",
    "Accuracy doesn’t always give the correct insight about your trained model\n",
    "1. Use the right evaluation metrics:\n",
    "    * Precision/Specificity: how many selected instances are relevant.\n",
    "    * Recall/Sensitivity: how many relevant instances are selected.\n",
    "    * F1 score: harmonic mean of precision and recall.\n",
    "    * MCC: correlation coefficient between the observed and predicted binary classifications.\n",
    "    * AUC: relation between true-positive rate and false positive rate.\n",
    "2. Resample the training set\n",
    "    * Under-sampling \n",
    "    * Over-sampling\n",
    "3. Use K-fold Cross-Validation in the right way\n",
    "4. Ensemble different resampled datasets\n",
    "5. Cluster the abundant class\n",
    "6. Design your own models ( XGBOOST)\n",
    "\n",
    "## PCA Dimensionality Reduction:\n",
    "Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. transforming a large set of variables into a smaller one that still contains most of the information in the large set. \n",
    "Step by Step Explanation of PCA\n",
    "1. Standardize the datapoints\n",
    "2. Find the covariance matrix from the given datapoints\n",
    "3. Carry out eigen-value decomposition of the covariance matrix\n",
    "4. Sort the eigenvalues and eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "Two type of Linear Regression:\n",
    "* 1. Simple linear regression\n",
    "* 2. Multiple Linear Regression\n",
    "Some of those have been enhanced to handle the multi output case:\n",
    "In statistical data analysis we sometimes use a correlation coefficient to quantify the linear relationship between two variables. The Pearson correlation coefficient is a number between -1 and +1 that measures both the strength\n",
    "and direction of the linear relationship between two variables.1 indicates a strong positive relationship. -1 indicates a strong negative relationship. A result of zero indicates no relationship at all.\n",
    "### Negative Correlation : \n",
    "If variables X and Y have a negative correlation (or are negatively correlated), as X increases in value, Y will decrease; similarly, if X decreases in value, Y will increase\n",
    "###  Positive Correlation:\n",
    "A relationship between two variables in which both variables move in tandem—that is, in the same direction.\n",
    "## Mean squared error : \n",
    "Calculated distance between estimated and actual.\n",
    "The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.\n",
    "![](./i/image16.png)\n",
    "The smaller the means squared error, the closer you are to finding the line of best fit. The line equation is y=Mx+B. We want to find M (slope) and B (y-intercept) that minimizes the squared error.\n",
    "![](./i/image26.png)\n",
    "\n",
    "## Mean absolute error: \n",
    "Calculated absolute distance between estimated and actual\n",
    "## R² score : \n",
    "the coefficient of determination:\n",
    "A: calculate distance actual value to mean of values\n",
    "B: second draw a regression line and predict estimate value\n",
    "C: calculate distance estimate value to the mean\n",
    "![](./i/image42.png)\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "* 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "* 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "Interpretation of R²-Squared The most common interpretation of r-squared is how well the regression model fits the observed data. For example, an r-squared of 60% reveals that 60% of the data\n",
    "fit the regression model. Generally, a higher r-squared indicates a better fit for the model.A low r-squared figure is generally a bad sign for predictive models. \n",
    "\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "The definition of R-squared is the percentage of the response variable variation that is explained by a linear model.\n",
    "R-squared = Explained variation / Total variation\n",
    "R-squared is always between 0 and 100%.\n",
    "0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "100% indicates that the model explains all the variability of the response data around its mean.\n",
    "In general, the higher the R-squared, the better the model fits your data.\n",
    "![](./i/image18.png)\n",
    "\n",
    "\n",
    "There is a problem with the R-Square. The problem arises when we ask this question to ourselves.** Is it\n",
    "good to help as many independent variables as possible?**\n",
    "The answer is No because we understood that each independent variable should have a meaningful\n",
    "impact. But, even** if we add independent variables which are not meaningful**, will it improve R-Square\n",
    "value?\n",
    "Yes, this is the basic problem with R-Square. How many junk independent variables or important\n",
    "independent variable or impactful independent variable you add to your model, the R-Squared value will\n",
    "always increase. It will never decrease with the addition of a newly independent variable, whether it could\n",
    "be an impactful, non-impactful, or bad variable, so we need another way to measure equivalent RSquare, which penalizes our model with any junk independent variable.\n",
    "So, we calculate the Adjusted R-Square with a better adjustment in the formula of generic R-square.\n",
    "![](./i/image33.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Metrics\n",
    "\n",
    "There are two types of statistical hypotheses.\n",
    "\n",
    "• Null hypothesis. The null hypothesis, denoted by Ho, is usually the hypothesis that sample\n",
    "observations result purely from chance.\n",
    "\n",
    "• Alternative hypothesis. The alternative hypothesis, denoted by H1 or Ha, is the hypothesis\n",
    "that sample observations are influenced by some non-random cause.\n",
    "\n",
    "### Ho: P = 0.5 = Two samples mean are equal.\n",
    "### Ha: P != 0.5 = Two samples mean are not equal.\n",
    "## Confidence Interval\n",
    "A confidence interval (CI) is a range of values that’s likely to include a population value with\n",
    "a certain degree of confidence. It is often expressed a percent whereby a population means lies\n",
    "between an upper and lower interval.\n",
    "![](./i/image46.png)\n",
    "## Critical region\n",
    "From this discussion we come to a strong conclusion that the levels marked by the probabilities\n",
    "0.05 or 0.01 which determine the significance of an event are called Levels Of Significance and\n",
    "are always expressed in percentages as 5% level of significance or 1% level of significance. The\n",
    "corresponding regions are called Critical Regions.\n",
    "\n",
    "![](./i/image56.png)\n",
    "\n",
    "In a two-sided test the null hypothesis is rejected if the test statistic is either too small or too large.\n",
    "Thus the rejection region for such a test consists of two parts: one on the left and one on the right.\n",
    "![](./i/image6.png)\n",
    "\n",
    "## P-Value\n",
    "The strength of evidence in support of a null hypothesis is measured by the P-value.\n",
    "If the P-value is less than the significance level, we reject the null hypothesis or Region of acceptance.\n",
    "\n",
    "## T-test:\n",
    "Compare two groups:\n",
    "\n",
    "Test the null hypothesis that two populations has the same average.\n",
    "\n",
    "t-test is same as Z test but When the n1 or n2 is less than 30 we use the T-test instead of the Z-test.\n",
    "![](./i/image34.png)\n",
    "\n",
    "## The ANOVA test \n",
    "assesses whether the averages of more than two groups are statistically different\n",
    "from each other. This analysis is appropriate for comparing the averages of a numerical variable\n",
    "for more than two categories of a categorical variable.\n",
    "\n",
    "How ANOVA works?\n",
    "\n",
    "• Check sample sizes: equal number of observation in each group\n",
    "\n",
    "• Calculate Mean Square for each group (MS) (SS of group/level-1); level-1 is a degree of\n",
    "freedom (df) for a group\n",
    "\n",
    "• Calculate Mean Square error (MSE) (SS error/df of residuals)\n",
    "\n",
    "• Calculate F-value (MS of group/MSE)\n",
    "\n",
    "## Z-score and Z test\n",
    "A Z-score is a numerical measurement used in statistics of a value’s relationship to the mean\n",
    "(average) of a group of values, measured in terms of standard deviations from the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between AI, Data Science, ML, and DL?\n",
    "\n",
    "Artificial Intelligence: AI is purely math and scientific exercise, but when it became computational, it started to solve human problems formalized into a subset of computer science. Artificial intelligence has changed the original computational statistics paradigm to the modern idea that machines could mimic actual human capabilities, such as decision making and performing more “human” tasks. Modern AI into two categories\n",
    "1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & business transactions\n",
    "2. Applied AI - driverless/ Autonomous car or machine smartly trade stocks\n",
    "\n",
    "Machine Learning: Instead of engineers “teaching” or programming computers to have what they need to carry out tasks, that perhaps computers could teach themselves – learn something without being explicitly programmed to do so. ML is a form of AI where based on more data, and they can change actions and response, which will make more efficient, adaptable and scalable. e.g., navigation apps and recommendation engines.\n",
    "Machine learning is the scientific study of algorithms and statistical models that computer systems use to\n",
    "effectively perform a specific task without using explicit instructions, relying on patterns and inference\n",
    "Instead. Building a model by learning the patterns of historical data with some relationship between data to make\n",
    "a data-driven prediction.\n",
    "## Classified into:\n",
    "### Supervised learning : \n",
    "In a supervised learning model, the algorithm learns on a labeled dataset, to generate reasonable\n",
    "predictions for the response to new data. (Forecasting outcome of new data)\n",
    "* Regression\n",
    "* Classification\n",
    "### Unsupervised learning:\n",
    "An unsupervised model, in contrast, provides unlabelled data that the algorithm tries to make sense of by extracting features, co-occurrence and underlying patterns on its own. We use unsupervised learning for\n",
    "* Clustering\n",
    "* Anomaly detection\n",
    "* Association\n",
    "* Autoencoders\n",
    "\n",
    "\n",
    "### Reinforcement learning:\n",
    "Reinforcement learning is less supervised and depends on the learning agent in determining the output\n",
    "solutions by arriving at different possible ways to achieve the best possible solution.\n",
    "Data Science: Data science has many tools, techniques, and algorithms called from these fields, plus\n",
    "others –to handle big data.\n",
    "\n",
    "The goal of data science, somewhat similar to machine learning, is to make accurate predictions and to\n",
    "automate and perform transactions in real-time, such as purchasing internet traffic or automatically\n",
    "generating content.\n",
    "\n",
    "Data science relies less on math and coding and more on data and building new systems to process the\n",
    "data. Relying on the fields of data integration, distributed architecture, automated machine learning, data\n",
    "visualization, data engineering, and automated data-driven decisions, data science can cover an entire\n",
    "spectrum of data processing, not only the algorithms or statistics related to data.\n",
    "Deep Learning: It is a technique for implementing ML.\n",
    "\n",
    "ML provides the desired output from a given input, but DL reads the input and applies it to another data.\n",
    "In ML, we can easily classify the flower based upon the features. Suppose you want a machine to look at\n",
    "an image and determine what it represents to the human eye, whether a face, flower, landscape, truck,\n",
    "building, etc.\n",
    "\n",
    "Machine learning is not sufficient for this task because machine learning can only produce an output from\n",
    "a data set – whether according to a known algorithm or based on the inherent structure of the data. You\n",
    "might be able to use machine learning to determine whether an image was of an “X” – a flower, say – and\n",
    "it would learn and get more accurate. But that output is binary (yes/no) and is dependent on the\n",
    "algorithm, not the data. In the image recognition case, the outcome is not binary and not dependent on\n",
    "the algorithm.\n",
    "\n",
    "The neural network performs MICRO calculations with computational on many layers. Neural networks\n",
    "also support weighting data for ‘confidence. These results in a probabilistic system, vs. deterministic, and\n",
    "can handle tasks that we think of as requiring more ‘human-like’ judgment.\n",
    "\n",
    "![](./i/image11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the general architecture of Machine learning.\n",
    "\n",
    "![](./i/image4.png)\n",
    "\n",
    "## Business understanding: \n",
    "Understand the give use case, and also, it's good to know more about the\n",
    "domain for which the use cases are built.\n",
    "## Data Acquisition and Understanding: \n",
    "Data gathering from different sources and understanding the data. Cleaning the data, handling the missing data if any, data wrangling, and EDA( Exploratory data analysis).\n",
    "## Modeling: \n",
    "Feature Engineering - scaling the data, feature selection - not all features are important. We use the backward elimination method, correlation factors, PCA and domain knowledge to select the features. Model Training based on trial and error method or by experience, we select the algorithm and train with the selected features.\n",
    "Model evaluation Accuracy of the model , confusion matrix and cross-validation.\n",
    "\n",
    "If accuracy is not high, to achieve higher accuracy, we tune the model...either by changing the algorithm used or by feature selection or by gathering more data, etc.\n",
    "## Deployment: \n",
    "Once the model has good accuracy, we deploy the model either in the cloud or Rasberry py or any other place. Once we deploy, we monitor the performance of the model.if its good...we go live with the model or reiterate the all process until our model performance is good.\n",
    "\n",
    "It's not done yet!!!\n",
    "\n",
    "What if, after a few days, our model performs badly because of new data. In that case, we do all the\n",
    "process again by collecting new data and redeploy the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Linear Regression?\n",
    "Linear Regression tends to establish a relationship between a dependent variable(Y) and one or more\n",
    "independent variable(X) by finding the best fit of the straight line.\n",
    "The equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept\n",
    "In the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x.' There is no straight line that\n",
    "runs through all the data points. So, the objective here is to fit the best fit of a straight line that will try to\n",
    "minimize the error between the expected and actual value.\n",
    "\n",
    "![](./i/image55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS Stats Model (Ordinary Least Square):\n",
    "OLS is a stats model, which will help us in identifying the more significant features that can has an\n",
    "influence on the output. OLS model in python is executed as:\n",
    "lm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()\n",
    "And we get the output as below,\n",
    "![](./i/image39.png)\n",
    "The higher the t-value for the feature, the more significant the feature is to the output variable. And\n",
    "also, the p-value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero\n",
    "significance on the target variable.). If the p-value is less than 0.05(95% confidence interval) for a\n",
    "feature, then we can consider the feature to be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Support Vector Regression? Difference between SVR and a simple regression model?\n",
    "In simple linear regression, try to minimize the error rate. But in SVR, we try to fit the error within\n",
    "a certain threshold.\n",
    "Main Concepts:\n",
    "* Boundary\n",
    "* Kernel\n",
    "* Support Vector\n",
    "* Hyper Plane\n",
    "\n",
    "Blueline: Hyper Plane; Red Line: Boundary-Line\n",
    "\n",
    "![](./i/image7.png)\n",
    "\n",
    "\n",
    "![](./i/image29.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Our best fit line is the one where the hyperplane has the maximum number of points.\n",
    "We are trying to do here is trying to decide a decision boundary at ‘e’ distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are within that\n",
    "boundary line.  \n",
    "\n",
    "![](./i/image54.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is ZCA Whitening?\n",
    "## Answer:\n",
    "Zero Component Analysis:\n",
    "    \n",
    "Making the covariance matrix as the Identity matrix is called whitening. This will remove the first and second-order statistical structure.\n",
    "ZCA transforms the data to zero means and makes the features linearly independent of each other In some image analysis applications, especially when working with images of the color and tiny type, it is frequently interesting to apply some whitening to the data before, e.g. training a classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression?\n",
    "The logistic regression technique involves the dependent variable, which can be represented in the binary (0 or 1, true or false, yes or no) values, which means that the outcome could only be in either one form of two. For example, it can be utilized when we need to find the probability of a successful or failed event.\n",
    "![](./i/image3.png)\n",
    "Logistic Regression is used when the dependent variable (target) is categorical.\n",
    "![](./i/image40.png)\n",
    "\n",
    "If ‘Z’ goes to infinity, Y(predicted) will become 1, and if ‘Z’ goes to negative infinity, Y(predicted) will become 0.\n",
    "The output from the hypothesis is the estimated probability. This is used to infer how confident can predicted value be actual value when given an input X.\n",
    "## Cost Function\n",
    "![](./i/image23.png)\n",
    "\n",
    "This implementation is for binary logistic regression. For data with more than 2 classes, softmax regression has to be used.\n",
    "\n",
    "## Difference between logistic and linear regression?\n",
    "Linear and Logistic regression are the most basic forms of regression which are commonly used. The essential difference between these two is that Logistic regression is used when the dependent variable is binary. In contrast, Linear regression is used when the dependent variable is continuous, and the nature of the regression line is linear.\n",
    "Key Differences between Linear and Logistic Regression\n",
    "Linear regression models data using continuous numeric values. As against, logistic regression models\n",
    "the data in the binary values.\n",
    "\n",
    "Linear regression requires to establish the linear relationship among dependent and independent\n",
    "variables, whereas it is not necessary for logistic regression.\n",
    "In linear regression, the independent variables can be correlated with each other. On the contrary, in\n",
    "logistic regression, the variables must not be correlated with each other.\n",
    "\n",
    "![](./i/image36.png)\n",
    "\n",
    "## Why we can’t do a classification problem using Regression?\n",
    "With linear regression you fit a polynomial through the data - say, like on the example below, we fit a straight line through {tumor size, tumor type} sample set:Above, malignant tumors get 1, and non-malignant ones get 0, and the green line is our hypothesis\n",
    "h(x). \n",
    "\n",
    "To make predictions, we may say that for any given tumor size x, if h(x) gets bigger than 0.5,\n",
    "We predict malignant tumors. Otherwise, we predict benignly.\n",
    "It looks like this way, we could correctly predict every single training set sample, but now let's change\n",
    "the task a bit.\n",
    "Intuitively it's clear that all tumors larger than a certain threshold are malignant. So let's add another sample\n",
    "with huge tumor size, and run linear regression again:\n",
    "![](./i/image20.png)\n",
    "![](./i/image47.png)\n",
    "\n",
    "Now our h(x)>0.5→malignant doesn't work anymore. To keep making correct predictions, we need\n",
    "to change it to h(x)>0.2 or something - but that is not how the algorithm should work.\n",
    "We cannot change the hypothesis each time a new sample arrives. Instead, we should learn it off the\n",
    "training set data, and then (using the hypothesis we've learned) make correct predictions for the data\n",
    "We haven't seen each other before.\n",
    "**Linear regression is unbounded.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is a decision tree full explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Random Forest Algorithm?\n",
    "## Answer:\n",
    "Random Forest is an ensemble machine learning algorithm that follows the bagging technique. The base estimators in the random forest are decision trees. Random forest randomly selects a set of features that are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "### Looking at it step-by-step, this is what a random forest model does:\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "\n",
    "To sum up, the Random forest randomly selects data points and features and builds multiple trees\n",
    "(Forest).\n",
    "Random Forest is used for feature importance selection. The attribute (.feature_importances_) is used to find feature importance.\n",
    "\n",
    "### Some Important Parameters:-\n",
    "1. n_estimators:- It defines the number of decision trees to be created in a random forest.\n",
    "2. criterion:- \"Gini\" or \"Entropy.\"\n",
    "3. min_samples_split:- Used to define the minimum number of samples required in a leaf node before a split is attempted\n",
    "4. max_features: -It defines the maximum number of features allowed for the split in each decision tree.\n",
    "5. n_jobs:- The number of jobs to run in parallel for both fit and predict. Always keep (-1) to use all the cores for parallel processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is SVM Classification?\n",
    "SVM or Large margin classifier is a supervised learning algorithm that uses a powerful technique\n",
    "called SVM for classification.\n",
    "## We have two types of SVM classifiers:\n",
    "### Linear SVM: \n",
    "In Linear SVM, the data points are expected to be separated by some apparent\n",
    "gap. Therefore, the SVM algorithm predicts a straight hyperplane dividing the two classes. The\n",
    "hyperplane is also called as maximum margin hyperplane\n",
    "![](./i/image48.png)\n",
    "![](./i/image37.png)\n",
    "\n",
    "### Non-Linear SVM: \n",
    "It is possible that our data points are not linearly separable in a p-dimensional space, but can be linearly separable in a higher dimension. Kernel tricks make it\n",
    "possible to draw nonlinear hyperplanes. Some standard kernels are a) Polynomial Kernel b) RBF\n",
    "kernel(mostly used)\n",
    "![](./i/image1.png)\n",
    "\n",
    "#### Advantages of SVM classifier:\n",
    "1) SVMs are effective when the number of features is quite large.\n",
    "2) It works effectively even if the number of features is greater than the number of samples.\n",
    "3) Non-Linear data can also be classified using customized hyperplanes built by using kernel tricks.\n",
    "4) It is a robust model to solve prediction problems since it maximizes margin.\n",
    "#### Disadvantages of SVM classifier:\n",
    "1) The biggest limitation of the Support Vector Machine is the choice of the kernel. The wrong choice of the kernel can lead to an increase in error percentage.\n",
    "2) With a greater number of samples, it starts giving poor performances.\n",
    "3) SVMs have good generalization performance, but they can be extremely slow in the test phase.\n",
    "4) SVMs have high algorithmic complexity and extensive memory requirements due to the use of quadratic programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is RandomizedSearchCV?\n",
    "## Answer:\n",
    "Randomized search CV is used to perform a random search on hyperparameters. Randomized\n",
    "search CV uses a fit and score method, predict proba, decision_func, transform, etc..,\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated\n",
    "search over parameter settings.\n",
    "\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of\n",
    "parameter settings is sampled from the specified distributions. The number of parameter settings that\n",
    "are tried is given by n_iter.\n",
    "##  Code Example :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:00:02.596542Z",
     "start_time": "2021-07-31T03:59:57.504234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.195254015709299, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                              random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                     penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is GridSearchCV?\n",
    "Grid search is the process of performing hyperparameter tuning to determine the optimal values for\n",
    "a given model.\n",
    "CODE Example:-\n",
    "\n",
    "\n",
    "Grid search runs the model on all the possible range of hyperparameter values and outputs the best\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:02:04.152761Z",
     "start_time": "2021-07-31T04:02:04.095809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_C',\n",
       " 'param_kernel',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:03:45.026496Z",
     "start_time": "2021-07-31T04:03:37.082635Z"
    }
   },
   "source": [
    "# What is BaysianSearchCV?\n",
    "Answer:\n",
    "Bayesian search, in contrast to the grid and random search, keeps track of past evaluation results,\n",
    "which they use to form a probabilistic model mapping hyperparameters to a probability of a score on\n",
    "the objective function.\n",
    "![](./i/image44.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:04:38.839078Z",
     "start_time": "2021-07-31T04:03:57.969164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anemati\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass return_X_y=True as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-optimize\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.75,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "        'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'degree': Integer(1,8),\n",
    "        'kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "    },\n",
    "    n_iter=32,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# executes bayesian optimization\n",
    "_ = opt.fit(X_train, y_train)\n",
    "\n",
    "# model can be saved, used for predictions or scoring\n",
    "print(opt.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
